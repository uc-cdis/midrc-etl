{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import sys\n",
    "import random\n",
    "import uuid\n",
    "import hashlib\n",
    "from pathlib import PosixPath, Path\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd = random.Random()\n",
    "rnd.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions = [\n",
    "    # \"2021/06\",\n",
    "    # \"2021/07\",\n",
    "    \"2021/08\",\n",
    "    \"2021/0827\",\n",
    "    # \"packages_acrimage/PETAL\",\n",
    "    \"2021/09\",\n",
    "    \"2021/10/batch6\",\n",
    "    \"2021/10/batch7\",\n",
    "    \"2021/11\",\n",
    "    # \"packages_acrimage/2021/ACRPETAL_20211220\",\n",
    "    # \"packages_acrimage/2022/ACR_20220107\",\n",
    "    # \"ACR_20211115\",\n",
    "    # \"ACR_20220107\",\n",
    "    # \"packages_ACR_20220218\",\n",
    "]\n",
    "\n",
    "submission_path = PosixPath(\n",
    "    \"~/CTDS/projects/midrc/ssot-s3/replicated-data-acr/acrclinical\"\n",
    ").expanduser()\n",
    "\n",
    "to_index_path = submission_path / \"..\" / \"to_index_acr\"\n",
    "(to_index_path/\"open\").mkdir(parents=True, exist_ok=True)\n",
    "(to_index_path/\"seq\").mkdir(parents=True, exist_ok=True)\n",
    "(to_index_path/\"remove\").mkdir(parents=True, exist_ok=True)\n",
    "(to_index_path/\"missing\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "sequestration_master_file_path = PosixPath(\n",
    "    \"~/CTDS/projects/midrc/indexing-data/sequestration/master_sequestration_locations_31927_2022-09-07.tsv\"\n",
    ").expanduser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA = PosixPath(\"~/CTDS/projects/midrc/ssot-s3\").expanduser()\n",
    "\n",
    "# for RemoveHeads files, they have the same structure and needs to remove imaging_study\n",
    "# which is `submitter_id` in the format of\n",
    "# <case>_<study_id>\n",
    "remove_heads_files = RAW_DATA.glob(\"**/RemoveHeads*.txt\")\n",
    "remove_heads_studies = map(lambda v: pd.read_csv(v, sep=\"\\t\"), remove_heads_files)\n",
    "remove_heads_studies = map(lambda v: v[\"submitter_id\"] \\\n",
    "        .str.split(\"_\", expand=True) \\\n",
    "        .rename(columns={0: \"case_id\", 1: \"study_id\"})[[\"study_id\"]],\n",
    "    remove_heads_studies)\n",
    "remove_heads_studies = pd.concat(remove_heads_studies).reset_index(drop=True)\n",
    "\n",
    "# same thing for different format of deletion files\n",
    "# there are two different formats: one for imaging_study and one for images :facepalm:\n",
    "# this needs some column renaming\n",
    "rename_columns = {\n",
    "    \"*type\": \"type\",\n",
    "    \"*submitter_id\": \"submitter_id\",\n",
    "    \"study_uid\": \"study_id\",\n",
    "}\n",
    "\n",
    "deletion_imaging_study_files = RAW_DATA.glob(\"**/deletion_imaging_*.tsv\")\n",
    "deletion_imaging_study_studies = map(lambda v: pd.read_csv(v, sep=\"\\t\") \\\n",
    "        .rename(columns=rename_columns),\n",
    "    deletion_imaging_study_files)\n",
    "deletion_imaging_study_studies = map(lambda v: v[[\"study_id\"]], deletion_imaging_study_studies)\n",
    "\n",
    "deletion_imaging_study_studies = pd.concat(deletion_imaging_study_studies).reset_index(drop=True)\n",
    "\n",
    "studies_to_delete = pd.concat([remove_heads_studies, deletion_imaging_study_studies]).reset_index(drop=True)\n",
    "match_studies_to_delete = studies_to_delete[\"study_id\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_master = {}\n",
    "with open(sequestration_master_file_path) as sequestration_master_file:\n",
    "    reader = csv.DictReader(sequestration_master_file, delimiter=\"\\t\")\n",
    "\n",
    "    for row in reader:\n",
    "        seq_master[row[\"case_ids\"]] = row[\"dataset\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_path = Path(submission_path) / submission\n",
    "\n",
    "image_manifest_file = list(\n",
    "    chain(\n",
    "        sub_path.glob(\"**/CIRR*.txt\"),\n",
    "        sub_path.glob(\"**/*image_manifest*.txt\"),\n",
    "        sub_path.glob(\"**/image_*.txt\"),\n",
    "        sub_path.glob(\"image_*.tsv\"),\n",
    "        sub_path.glob(\"*_instance_*.tsv\"),\n",
    "    )\n",
    ")\n",
    "# studies_file = list(SUBMISSION_PATH.glob(\"*imaging_study_*.tsv\"))[0]\n",
    "series_files = list(\n",
    "    chain(\n",
    "        sub_path.glob(\"**/*_series_*.txt\"),\n",
    "        sub_path.glob(\"*_series_*.tsv\"),\n",
    "    )\n",
    ")\n",
    "list(sub_path.glob(\"**/*_series_*.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for submission in submissions:\n",
    "    print(submission)\n",
    "    # packages_path = Path(output_path) / submission\n",
    "    # packages_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    sub_path = Path(submission_path) / submission\n",
    "\n",
    "    image_manifest_file = list(\n",
    "        chain(\n",
    "            sub_path.glob(\"**/CIRR*.txt\"),\n",
    "            sub_path.glob(\"**/*image_manifest*.txt\"),\n",
    "            sub_path.glob(\"**/image_*.txt\"),\n",
    "            sub_path.glob(\"image_*.tsv\"),\n",
    "            sub_path.glob(\"*_instance_*.tsv\"),\n",
    "        )\n",
    "    )\n",
    "    # studies_file = list(SUBMISSION_PATH.glob(\"*imaging_study_*.tsv\"))[0]\n",
    "    series_files = list(\n",
    "        chain(\n",
    "            sub_path.glob(\"**/*_series_*.txt\"),\n",
    "            sub_path.glob(\"*_series_*.tsv\"),\n",
    "        )\n",
    "    )\n",
    "    print(list(series_files))\n",
    "    # instance_files = list(SUBMISSION_PATH.glob(\"*_instance_*.tsv\"))\n",
    "\n",
    "    rename_columns = {\n",
    "        \"case_ids\": \"case_id\",\n",
    "        \"Subject_ID\": \"case_id\",\n",
    "        \"series_uid\": \"series_id\",\n",
    "        \"dr_exams.submitter_id\": \"study_id\",\n",
    "        \"radiography_exam.submitter_id\": \"study_id\",\n",
    "        \"radiography_exams.submitter_id\": \"study_id\",\n",
    "        \"ct_scan.submitter_id\": \"study_id\",\n",
    "        \"ct_scans.submitter_id\": \"study_id\",\n",
    "        \"mr_exams.submitter_id\": \"study_id\",\n",
    "        \"nm_exams.submitter_id\": \"study_id\",\n",
    "        \"pt_scans.submitter_id\": \"study_id\",\n",
    "        \"pr_exams.submitter_id\": \"study_id\",\n",
    "        \"rf_exams.submitter_id\": \"study_id\",\n",
    "        \"series-submitter\": \"submitter_id\",\n",
    "        \"imaging_studies.submitter_id\": \"study_id\",\n",
    "    }\n",
    "\n",
    "    series = map(\n",
    "        lambda v: pd.read_csv(v, sep=\"\\t\").rename(columns=rename_columns), series_files\n",
    "    )\n",
    "    series = pd.concat(series, ignore_index=True).reset_index(drop=True)\n",
    "\n",
    "    # series[\"study_id\"] = series[\"study_id\"].apply(lambda v: v.split(\"_\")[1])\n",
    "\n",
    "    series = series[[\"series_id\", \"study_id\", \"case_id\"]].drop_duplicates()\n",
    "\n",
    "    # series\n",
    "\n",
    "    rename_columns = {\n",
    "        \"case_ids\": \"case_id\",\n",
    "        \"study_uid\": \"study_id\",\n",
    "        \"ct_scans.submitter_id\": \"study_id\",\n",
    "        \"radiography_exam.submitter_id\": \"study_id\",\n",
    "        \"series_uid\": \"series_id\",\n",
    "        \"series.submitter_id\": \"series_id\",\n",
    "        \"cr_series.submitter_id\": \"series_id\",\n",
    "        \"ct_series.submitter_id\": \"series_id\",\n",
    "        \"dx_series.submitter_id\": \"series_id\",\n",
    "        \"*md5sum\": \"md5sum\",\n",
    "        \"mdsum\": \"md5sum\",\n",
    "        \"*file_name\": \"file_name\",\n",
    "        \"*file_size\": \"file_size\",\n",
    "        \"submitter_id\": \"instance_id\",\n",
    "        \"object_id\": \"instance_id\",\n",
    "    }\n",
    "\n",
    "    instances = map(\n",
    "        lambda v: pd.read_csv(v, sep=\"\\t\").rename(columns=rename_columns),\n",
    "        image_manifest_file,\n",
    "    )\n",
    "    instances = pd.concat(instances, ignore_index=True).reset_index(drop=True)\n",
    "\n",
    "    # instances[\"series_id\"] = instances[\"series_id\"].apply(lambda v: v.split(\"_\")[1])\n",
    "    # instances[\"study_id\"] = instances[\"study_id\"].apply(lambda v: v.split(\"_\")[1])\n",
    "\n",
    "    instances = instances.merge(series, on=[\"case_id\", \"series_id\"])\n",
    "\n",
    "    if instances[\"file_size\"].dtype == np.dtype(\"O\"):\n",
    "        instances[\"file_size\"] = instances[\"file_size\"].apply(lambda v: locale.atoi(v))\n",
    "    instances = instances[\n",
    "        [\n",
    "            \"file_name\",\n",
    "            \"file_size\",\n",
    "            \"md5sum\",\n",
    "            \"case_id\",\n",
    "            \"study_id\",\n",
    "            \"series_id\",\n",
    "            \"instance_id\",\n",
    "            \"storage_urls\",\n",
    "        ]\n",
    "    ].drop_duplicates()\n",
    "\n",
    "    # instances\n",
    "\n",
    "    list_of_packages = []\n",
    "\n",
    "    break\n",
    "\n",
    "    # for _, row in instances.iterrows():\n",
    "    #     case_id = row[\"case_id\"]\n",
    "    #     study_id = row[\"study_id\"]\n",
    "    #     series_id = row[\"series_id\"]\n",
    "\n",
    "    #     series_path = f\"./cases/{case_id}/{study_id}/{series_id}.tsv\\n\"\n",
    "    #     if series_path not in list_of_packages:\n",
    "    #         list_of_packages.append(series_path)\n",
    "\n",
    "    #     folder = packages_path / \"cases\" / case_id / study_id\n",
    "\n",
    "    #     folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    #     series_file = folder / f\"{series_id}.tsv\"\n",
    "    #     series_file_exist = series_file.exists()\n",
    "\n",
    "    #     with open(series_file, mode=\"a\") as f:\n",
    "    #         fieldnames = [\n",
    "    #             \"file_name\",\n",
    "    #             \"file_size\",\n",
    "    #             \"md5sum\",\n",
    "    #             \"case_id\",\n",
    "    #             \"study_id\",\n",
    "    #             \"series_id\",\n",
    "    #             \"instance_id\",\n",
    "    #             \"storage_urls\",\n",
    "    #         ]\n",
    "    #         writer = csv.DictWriter(f, delimiter=\"\\t\", fieldnames=fieldnames)\n",
    "\n",
    "    #         if not series_file_exist:\n",
    "    #             writer.writeheader()\n",
    "    #         writer.writerow(row.to_dict())\n",
    "\n",
    "    # with open(packages_path / \"packages.txt\", \"w\") as f:\n",
    "    #     f.writelines(list_of_packages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for submission in submissions:\n",
    "    package_files = submission_path / submission / \"packages\"\n",
    "\n",
    "    print(package_files)\n",
    "\n",
    "    open_packages = []\n",
    "    seq_packages = []\n",
    "    to_remove_packages = []\n",
    "    missing_packages = []\n",
    "\n",
    "    for package_filepath in package_files.iterdir():\n",
    "        with open(package_filepath) as package_file:\n",
    "            reader = csv.DictReader(package_file, delimiter=\"\\t\")\n",
    "            for row in reader:\n",
    "                item = row\n",
    "\n",
    "                file_name = item[\"file_name\"]\n",
    "                case_id, study_id, _ = file_name.split(\"/\")\n",
    "\n",
    "                # in case study_id have some prefixes\n",
    "                study_id = study_id.split(\"_\")[-1]\n",
    "\n",
    "                package_contents = json.loads(item[\"package_contents\"].replace(\"'\", \"\\\"\"))\n",
    "                for p in package_contents:\n",
    "                    p[\"size\"] = int(p[\"size\"])\n",
    "                \n",
    "                item[\"package_contents\"] = json.dumps(package_contents)\n",
    "\n",
    "                dataset = seq_master.get(case_id, None)\n",
    "\n",
    "                if dataset == \"Open\":\n",
    "                    bucket = \"s3://open-data-midrc/\"\n",
    "                    authz = json.dumps([\"/programs/Open/projects/A1\"])\n",
    "                elif dataset == \"Seq\":\n",
    "                    bucket = \"s3://sequestered-data-midrc/\"\n",
    "                    authz = json.dumps([\"/programs/SEQ_Open/projects/A3\"])\n",
    "                else:\n",
    "                    authz = \"\"\n",
    "                    bucket = \"\"\n",
    "\n",
    "                item[\"authz\"] = authz\n",
    "                item[\"url\"] = f\"{bucket}{item['url']}\"\n",
    "\n",
    "                if study_id in match_studies_to_delete:\n",
    "                    to_remove_packages.append(item)\n",
    "                    continue\n",
    "\n",
    "                m = hashlib.md5()\n",
    "                m.update(f\"{item['md5']}{item['size']}\".encode('utf-8'))\n",
    "                item[\"guid\"] = f\"dg.MD1R/{uuid.UUID(m.hexdigest(), version=4)}\"\n",
    "                if dataset == \"Open\":\n",
    "                    open_packages.append(item)\n",
    "                elif dataset == \"Seq\":\n",
    "                    seq_packages.append(item)\n",
    "                else:\n",
    "                    missing_packages.append(item)\n",
    "\n",
    "    datasets = [\n",
    "        (f\"open/new_packages_open_{submission.split('/')[-1].removeprefix('packages_')}.tsv\", open_packages),\n",
    "        (f\"seq/new_packages_seq_{submission.split('/')[-1].removeprefix('packages_')}.tsv\", seq_packages),\n",
    "        (f\"remove/new_packages_remove_{submission.split('/')[-1].removeprefix('packages_')}.tsv\", to_remove_packages),\n",
    "        (f\"missing/new_packages_missing_{submission.split('/')[-1].removeprefix('packages_')}.tsv\", missing_packages),\n",
    "    ]\n",
    "\n",
    "    fieldnames = [\n",
    "        \"record_type\",\n",
    "        \"guid\",\n",
    "        \"md5\",\n",
    "        \"size\",\n",
    "        \"authz\",\n",
    "        \"url\",\n",
    "        \"file_name\",\n",
    "        \"package_contents\",\n",
    "    ]\n",
    "\n",
    "    for filename, dataset in datasets:\n",
    "        if not dataset:\n",
    "            continue\n",
    "        with open(\n",
    "            to_index_path / filename,\n",
    "            \"w\",\n",
    "        ) as f:\n",
    "            writer = csv.DictWriter(f, delimiter=\"\\t\", fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "\n",
    "            for item in dataset:\n",
    "                writer.writerow(item)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d5c72d1243d6df0c85fac95eab025b92ad6f1709943deea63d571be8293a3eb7"
  },
  "kernelspec": {
   "display_name": "Python 3.10.3 64-bit ('midrc-etl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
